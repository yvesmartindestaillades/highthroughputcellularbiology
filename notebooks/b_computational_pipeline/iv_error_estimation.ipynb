{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error estimation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../../src')\n",
    "import util, config, plots\n",
    "import ipynbname\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The bootstrap always fits a binomial distribution, independent of the underlying distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Full dataset stats\n",
    "N, D = 10000, 1000\n",
    "data = np.concatenate(( (np.random.rand(N, D//2) < 0.02).astype(int), (np.random.rand(N, D//2) < 0.03).astype(int) ), axis=1)\n",
    "muts_count = np.sum(data, axis=0)\n",
    "\n",
    "\n",
    "# Stats on one column of data\n",
    "data_col = data[:, np.random.randint(0, D-1)]\n",
    "\n",
    "bootstrap_data = np.random.choice(data_col, size=(N, D), replace=True)\n",
    "\n",
    "muts_count_bs = np.sum(bootstrap_data, axis=0)\n",
    "\n",
    "# Make a Plotly subplot with the histogram of muts_count on the first row and the bootstrapping on the second row\n",
    "fig = make_subplots(rows=2, cols=1, shared_xaxes=True, vertical_spacing=0.02)\n",
    "\n",
    "fig.add_trace(go.Histogram(x=muts_count, histnorm='probability density', name='Original dataset' ), col=1, row=1)\n",
    "\n",
    "fig.add_trace(go.Histogram(x=muts_count_bs, histnorm='probability density', name='bootstrap data'), col=1, row=2)\n",
    "\n",
    "# Draw a binomial distribution on top of the histogram, with n=1000 and p estimated from the data\n",
    "from scipy.stats import binom\n",
    "x = np.arange(min(muts_count_bs), max(muts_count_bs)).astype(int)\n",
    "fig.add_trace(go.Scatter(x=x, y=binom.pmf(x, len(data_col), np.count_nonzero(data_col)/len(data_col)), mode='lines', name='binomial'), col=1, row=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 1000 # number of residues\n",
    "N = 100000 # number of reads\n",
    "\n",
    "def generate_dataset(N, r):\n",
    "    assert r % 2 == 0\n",
    "    true_mutation_rate = np.concatenate([np.random.beta(20, 500, r//2) , np.random.beta(0.2, 30, r//2)])\n",
    "    true_mutation_rate[true_mutation_rate < 0] = 0.001\n",
    "    data = np.where(true_mutation_rate > np.random.rand(N, len(true_mutation_rate)), 1, 0) \n",
    "    return data, true_mutation_rate\n",
    "    \n",
    "def generate_sample(data, size_sample):\n",
    "    return data[np.random.randint(0, N, size_sample)]\n",
    "\n",
    "data, true_mutation_rate = generate_dataset(N, r)\n",
    "\n",
    "plt.subplots(figsize=(13, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(true_mutation_rate, bins=10)\n",
    "plt.title('Distribution of the true mutation rate')\n",
    "plt.xlabel('Mutation rate')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(data.sum(axis=0)/N, 'o')\n",
    "plt.plot(true_mutation_rate, 'x')\n",
    "plt.xlabel('Residue')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(['Observed', 'True'])\n",
    "plt.title('Quality assessment of the dataset: Observed vs True Frequency')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_orc('../../bv/01_1_s22_reads_3043.orc')\n",
    "\n",
    "def process_data(data):\n",
    "    data = data.to_numpy(dtype=np.byte).astype(float)\n",
    "    data[data == 0] = 0\n",
    "    data[data < 0] = 128\n",
    "    data[data < 64] = 0\n",
    "    data[data > 0] = 1\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a test sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample is size_sample reads from data, randomly selected\n",
    "size_sample = 2000\n",
    "sample = generate_sample(data, size_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sample.sum(axis=0)/size_sample, 'o')\n",
    "plt.plot(data.sum(axis=0)/data.shape[0], 'x')\n",
    "plt.xlabel('Residue')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap vs binomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Bootstrap\n",
    "### Define bootstrap function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(sample, n=1000):\n",
    "    \"\"\"\n",
    "    Bootstrap the sample\n",
    "    \"\"\"\n",
    "    n, r = sample.shape\n",
    "    res = np.zeros((n, r))\n",
    "    for i in range(n):\n",
    "        res[i] = sample[np.random.randint(0, n, n)].sum(axis=0)\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict confidence intervals with bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_confidence_interval_bootstrap(sample, bootstrap_iterations=1000):\n",
    "    bs = bootstrap(sample, n=bootstrap_iterations)\n",
    "    lower_bound, upper_bound = np.percentile(bs, [2.5, 97.5], axis=0)/size_sample\n",
    "    return lower_bound, upper_bound\n",
    "    \n",
    "def plot_confidence_interval_bootstrap(sample, data, size_sample, lower_bound, upper_bound):\n",
    "    observed_freq = sample.sum(axis=0)/size_sample  \n",
    "    real_freq = data.sum(axis=0)/data.shape[0]  \n",
    "    ord = np.argsort(real_freq)\n",
    "    observed_freq = observed_freq[ord]\n",
    "    lower_bound = lower_bound[ord]\n",
    "    upper_bound = upper_bound[ord]\n",
    "    real_freq = real_freq[ord]\n",
    "    \n",
    "    plt.plot(lower_bound, label='Lower bound')\n",
    "    plt.plot(upper_bound, label='Upper bound')\n",
    "    plt.plot(observed_freq, label='Observed')\n",
    "    plt.plot(real_freq, label='True')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Residue')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Bootstrap confidence interval, N = '+size_sample)\n",
    "    \n",
    "plot_confidence_interval_bootstrap(sample, data, size_sample, *predict_confidence_interval_bootstrap(sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binomial distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict confidence intervals with binomial distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# start with the observed frequency, and the sample size, not the bitvector\n",
    "observed_freq = sample.sum(axis=0)/size_sample\n",
    "\n",
    "def predict_confidence_interval_binomial_distribution(observed_freq, size_sample):\n",
    "    \"\"\"\n",
    "    Predict the confidence interval using the binomial distribution\n",
    "    \"\"\"\n",
    "    lower_bound = observed_freq - 1.96*np.sqrt(observed_freq*(1-observed_freq)/size_sample)\n",
    "    upper_bound = observed_freq + 1.96*np.sqrt(observed_freq*(1-observed_freq)/size_sample)\n",
    "    return lower_bound, upper_bound\n",
    "    \n",
    "def plot_confidence_interval_binomial_distribution(observed_freq, data, size_sample, lower_bound, upper_bound):\n",
    "    true_freq = data.sum(axis=0)/data.shape[0]\n",
    "    ord = np.argsort(true_freq)\n",
    "    observed_freq = observed_freq[ord]\n",
    "    lower_bound = lower_bound[ord]\n",
    "    upper_bound = upper_bound[ord]\n",
    "    true_freq = true_freq[ord]\n",
    "    plt.plot(lower_bound, label='Lower bound')\n",
    "    plt.plot(upper_bound, label='Upper bound')\n",
    "    plt.plot(observed_freq, label='Observed')\n",
    "    plt.plot(true_freq, label='True')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Residue')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Binomial distribution confidence interval N = '+ size_sample)\n",
    "    \n",
    "plot_confidence_interval_binomial_distribution(observed_freq, data, size_sample, *predict_confidence_interval_binomial_distribution(observed_freq, size_sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare bootstrap and binomial distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observed_freq = sample.sum(axis=0)/size_sample\n",
    "bootstrap_lower_bound, bootstrap_upper_bound = predict_confidence_interval_bootstrap(sample)\n",
    "binomial_lower_bound, binomial_upper_bound = predict_confidence_interval_binomial_distribution(observed_freq, size_sample)\n",
    "\n",
    "real_freq = data.sum(axis=0)/data.shape[0]\n",
    "ord = np.argsort(real_freq)\n",
    "real_freq = real_freq[ord]\n",
    "observed_freq = observed_freq[ord]\n",
    "bootstrap_lower_bound = bootstrap_lower_bound[ord]\n",
    "bootstrap_upper_bound = bootstrap_upper_bound[ord]\n",
    "binomial_lower_bound = binomial_lower_bound[ord]\n",
    "binomial_upper_bound = binomial_upper_bound[ord]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(bootstrap_lower_bound, label='Bootstrap lower bound')\n",
    "plt.plot(bootstrap_upper_bound, label='Bootstrap upper bound')\n",
    "plt.plot(binomial_lower_bound, label='Binomial lower bound')\n",
    "plt.plot(binomial_upper_bound, label='Binomial upper bound')\n",
    "plt.plot(observed_freq, label='Observed')\n",
    "plt.plot(real_freq, label='True')\n",
    "plt.title('Comparison of the confidence intervals: Bootstrap vs Binomial, N = '+size_sample)\n",
    "plt.xlabel('Residue')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access bootstrap performance\n",
    "Predict a confidence interval for a sub-sample of the data, and make sure that the confidence interval contains the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials_per_dataset = 10\n",
    "bootstrap_iterations = 1000\n",
    "n_datasets = 10\n",
    "\n",
    "bootstrap_failures = []\n",
    "binomial_failures = []\n",
    "\n",
    "sample_sizes = [2000]\n",
    "\n",
    "for N in sample_sizes:\n",
    "    \n",
    "    print('N =', N)\n",
    "    for f in os.listdir('../../bv'):\n",
    "        \n",
    "        data = process_data(pd.read_orc('../../bv/'+f))\n",
    "        # generate a new dataset\n",
    "        true_mutation_rate = data.sum(axis=0)/data.shape[0]\n",
    "        \n",
    "\n",
    "        fail_bootstrap, fail_binomial = [], []\n",
    "        for _ in range(n_trials_per_dataset):\n",
    "            \n",
    "            # generate a new sample\n",
    "            sample = data[np.random.randint(0, data.shape[0], size=N)]\n",
    "            observed_freq = sample.sum(axis=0)/N\n",
    "            \n",
    "            # count failures bootstrap\n",
    "            lb_bs, ub_bs = predict_confidence_interval_bootstrap(sample, bootstrap_iterations=bootstrap_iterations)\n",
    "            fail_bootstrap.append(100*np.logical_or(ub_bs < true_mutation_rate, lb_bs > true_mutation_rate).sum()/data.shape[1])\n",
    "\n",
    "            # count failures binomial\n",
    "            lb_bin, ub_bin = predict_confidence_interval_binomial_distribution(observed_freq, N)\n",
    "            fail_binomial.append(100*np.logical_or(ub_bs < true_mutation_rate, lb_bs > true_mutation_rate).sum()/data.shape[1])\n",
    "\n",
    "        bootstrap_failures.append(fail_bootstrap)\n",
    "        binomial_failures.append(fail_binomial)\n",
    "        print(f'{f}: Binomial: {np.mean(binomial_failures[-1])}% failure rate')\n",
    "        print(f'{f}: Bootstrap: {np.mean(bootstrap_failures[-1])}% failure rate')\n",
    "        \n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            'Bootstrap': np.array(bootstrap_failures).flatten(), \n",
    "            'Binomial': np.array(binomial_failures).flatten(), \n",
    "            'Trial' : np.repeat(np.arange(n_trials_per_dataset),len(os.listdir('../../bv')))\n",
    "        })\n",
    "    df = pd.melt(df, id_vars=['Trial'], value_vars=['Bootstrap', 'Binomial'], var_name='Method', value_name='Failure rate (%)')\n",
    "\n",
    "    fig = px.box(df, x='Trial', y='Failure rate (%)', color='Method', title='Failure rate of the confidence interval methods for data {} and sample size {}'.format(f, N))\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The bootstrap method is equivalent the binomial distribution.\n",
    "\n",
    "Boostrapping isn't a super reliable method to get confidence intervals on the test set (it's not a good idea to use the test set to estimate the performance of the model). However, it's a good method to get confidence intervals on the training set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative methods performance assessment\n",
    "\n",
    "### Read dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = process_data(pd.read_orc('../../bv/01_1_S22_reads_3081.orc'))\n",
    "sample = data[np.random.randint(0, data.shape[0], size=N)]\n",
    "observed_freq = sample.sum(axis=0)/N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##  Wilson score \n",
    "### Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wilson(p, n, z = 1.96):\n",
    "    denominator = 1 + z**2/n\n",
    "    centre_adjusted_probability = p + z*z / (2*n)\n",
    "    adjusted_standard_deviation = np.sqrt((p*(1 - p) + z*z / (4*n)) / n)\n",
    "    \n",
    "    lower_bound = (centre_adjusted_probability - z*adjusted_standard_deviation) / denominator\n",
    "    upper_bound = (centre_adjusted_probability + z*adjusted_standard_deviation) / denominator\n",
    "    return (lower_bound, upper_bound)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict confidence intervals with Wilson score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confidence_interval_wilson(observed_freq, lower_bound, upper_bound):\n",
    "    ord = np.argsort(observed_freq)\n",
    "    observed_freq = observed_freq[ord]\n",
    "    lower_bound = lower_bound[ord]\n",
    "    upper_bound = upper_bound[ord]\n",
    "    plt.plot(lower_bound, label='Lower bound')\n",
    "    plt.plot(upper_bound, label='Upper bound')\n",
    "    plt.plot(observed_freq, label='True')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Residue')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Wilson confidence interval')\n",
    "    \n",
    "plot_confidence_interval_wilson(observed_freq, *wilson(observed_freq, size_sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clopper-Pearson \n",
    "### Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clopper_pearson(*args):\n",
    "    return sm.stats.proportion_confint(*args, method='beta')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict confidence intervals with Clopper-Pearson "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clopper_pearson(observed_freq, lower_bound, upper_bound):\n",
    "    ord = np.argsort(observed_freq)\n",
    "    observed_freq = observed_freq[ord]\n",
    "    lower_bound = lower_bound[ord]\n",
    "    upper_bound = upper_bound[ord]\n",
    "    plt.plot(lower_bound, label='Lower bound')\n",
    "    plt.plot(upper_bound, label='Upper bound')\n",
    "    plt.plot(observed_freq, label='True')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Residue')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Clopper-Pearson confidence interval')\n",
    "    \n",
    "plot_clopper_pearson(observed_freq, *clopper_pearson(observed_freq*size_sample, size_sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agresti-Coull\n",
    "### Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agresti_coull(*args):\n",
    "    return sm.stats.proportion_confint(*args, method='agresti_coull')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict confidence intervals with Agresti-Coull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_agresti_coull(observed_freq, data, lower_bound, upper_bound):\n",
    "    true_mutation_rate = data.sum(axis=0)/data.shape[0]\n",
    "    ord = np.argsort(true_mutation_rate)\n",
    "    true_mutation_rate = true_mutation_rate[ord]\n",
    "    observed_freq = observed_freq[ord]\n",
    "    lower_bound = lower_bound[ord]\n",
    "    upper_bound = upper_bound[ord]\n",
    "    plt.plot(lower_bound, label='Lower bound')\n",
    "    plt.plot(upper_bound, label='Upper bound')\n",
    "    plt.plot(observed_freq, label='Observed')\n",
    "    plt.plot(true_mutation_rate, label='True')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Residue')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Agresti-Coull confidence interval')\n",
    "    \n",
    "plot_agresti_coull(observed_freq, data, *agresti_coull(observed_freq*size_sample, size_sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def predict_confidence_interval_poisson(observed_freq, size_sample, alpha=0.05):\n",
    "    cov = size_sample\n",
    "    mut = observed_freq*size_sample\n",
    "    return 0.5*scipy.stats.chi2.ppf(alpha/2, df=2*mut)/cov, 0.5*scipy.stats.chi2.ppf(1-alpha/2, df=2*(mut+1))/cov\n",
    "\n",
    "def plot_poisson(observed_freq, data, lower_bound, upper_bound):\n",
    "    true_mutation_rate = data.sum(axis=0)/data.shape[0]\n",
    "    ord = np.argsort(true_mutation_rate)\n",
    "    true_mutation_rate = true_mutation_rate[ord]\n",
    "    observed_freq = observed_freq[ord]\n",
    "    lower_bound = lower_bound[ord]\n",
    "    upper_bound = upper_bound[ord]\n",
    "    plt.plot(lower_bound, label='Lower bound')\n",
    "    plt.plot(upper_bound, label='Upper bound')\n",
    "    plt.plot(observed_freq, label='Observed')\n",
    "    plt.plot(true_mutation_rate, label='True')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Residue')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Poisson confidence interval')\n",
    "\n",
    "plot_poisson(observed_freq, data, *predict_confidence_interval_poisson(observed_freq, size_sample))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perfomance assessment confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_trials_per_dataset = 10\n",
    "bootstrap_iterations = 1000\n",
    "n_datasets = 10\n",
    "methods = ['bootstrap', 'binomial', 'wilson', 'clopper_pearson', 'agresti_coull', 'poisson']\n",
    "\n",
    "for size_sample in [500, 1000, 2000, 3000, 5000, 10000]:\n",
    "    failures = {m:[] for m in methods}\n",
    "    size_ci = {m:[] for m in methods}\n",
    "\n",
    "    for f in os.listdir('../../bv'):\n",
    "        \n",
    "        # read dataset\n",
    "        data = process_data(pd.read_orc('../../bv/' + f))\n",
    "        true_mutation_rate = data.sum(axis=0)/data.shape[0]\n",
    "\n",
    "        fail = {m:[] for m in methods}\n",
    "        size = {m:[] for m in methods}\n",
    "        for _ in range(n_trials_per_dataset):\n",
    "            \n",
    "            # generate a new sample\n",
    "            sample = data[np.random.randint(0, data.shape[0], size=size_sample)]\n",
    "            observed_freq = sample.sum(axis=0)/size_sample\n",
    "            \n",
    "            # count failures bootstrap\n",
    "            lb_bs, ub_bs = predict_confidence_interval_bootstrap(sample, bootstrap_iterations=bootstrap_iterations)\n",
    "            fail['bootstrap'].append(100*np.logical_or(ub_bs < true_mutation_rate, lb_bs > true_mutation_rate).sum()/data.shape[1])\n",
    "            size['bootstrap'].append(np.mean(ub_bs - lb_bs))\n",
    "            \n",
    "            # count failures binomial\n",
    "            lb_bin, ub_bin = predict_confidence_interval_binomial_distribution(observed_freq, size_sample)\n",
    "            fail['binomial'].append(100*np.logical_or(ub_bs < true_mutation_rate, lb_bs > true_mutation_rate).sum()/data.shape[1])\n",
    "            size['binomial'].append(np.mean(ub_bin - lb_bin))\n",
    "            \n",
    "            # count Wilson failures\n",
    "            lb_wilson, ub_wilson = wilson(observed_freq, size_sample)\n",
    "            fail['wilson'].append(100*np.logical_or(ub_wilson < true_mutation_rate, lb_wilson > true_mutation_rate).sum()/data.shape[1])\n",
    "            size['wilson'].append(np.mean(ub_wilson - lb_wilson))\n",
    "            \n",
    "            # count Clopper-Pearson failures\n",
    "            lb_cp, ub_cp = clopper_pearson(observed_freq*size_sample, size_sample)\n",
    "            fail['clopper_pearson'].append(100*np.logical_or(ub_cp < true_mutation_rate, lb_cp > true_mutation_rate).sum()/data.shape[1])\n",
    "            size['clopper_pearson'].append(np.mean(ub_cp - lb_cp))\n",
    "            \n",
    "            # count Agresti-Coull failures\n",
    "            lb_ac, ub_ac = agresti_coull(observed_freq*size_sample, size_sample)\n",
    "            fail['agresti_coull'].append(100*np.logical_or(ub_ac < true_mutation_rate, lb_ac > true_mutation_rate).sum()/data.shape[1])\n",
    "            size['agresti_coull'].append(np.mean(ub_ac - lb_ac))\n",
    "            \n",
    "            # count Poisson failures\n",
    "            lb_poisson, ub_poisson = predict_confidence_interval_poisson(observed_freq, size_sample)\n",
    "            fail['poisson'].append(100*np.logical_or(ub_poisson < true_mutation_rate, lb_poisson > true_mutation_rate).sum()/data.shape[1])\n",
    "            size['poisson'].append(np.mean(ub_poisson - lb_poisson))\n",
    "\n",
    "        for m in methods:\n",
    "            failures[m].append(fail[m])\n",
    "            size_ci[m].append(size[m])\n",
    "        #    print(f'{n_dataset}: {m}: {np.mean(failures[m][-1])}% failure rate')\n",
    "        #print('----------------------------------------------------------')\n",
    "        \n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            'Bootstrap': np.array(failures['bootstrap']).flatten(), \n",
    "            'Binomial': np.array(failures['binomial']).flatten(), \n",
    "            'Wilson': np.array(failures['wilson']).flatten(), \n",
    "            'Clopper-Pearson': np.array(failures['clopper_pearson']).flatten(), \n",
    "            'Agresti-Coull': np.array(failures['agresti_coull']).flatten(), \n",
    "            'Poisson': np.array(failures['poisson']).flatten(), \n",
    "            'Trial' : np.repeat(np.arange(n_trials_per_dataset), len(os.listdir('../../bv')))\n",
    "        })\n",
    "    df = pd.melt(df, id_vars=['Trial'], value_vars=[c for c in df.columns if not c == 'Trial'], var_name='Method', value_name='Failure rate (%)')\n",
    "\n",
    "    fig = px.box(df, x='Trial', y='Failure rate (%)', color='Method', title='Failure rate of the confidence interval methods. N = {}'.format(size_sample))\n",
    "    fig.show()\n",
    "    \n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            'Bootstrap': np.array(size_ci['bootstrap']).flatten(), \n",
    "            'Binomial': np.array(size_ci['binomial']).flatten(), \n",
    "            'Wilson': np.array(size_ci['wilson']).flatten(), \n",
    "            'Clopper-Pearson': np.array(size_ci['clopper_pearson']).flatten(), \n",
    "            'Agresti-Coull': np.array(size_ci['agresti_coull']).flatten(), \n",
    "            'Poisson': np.array(size_ci['poisson']).flatten(), \n",
    "            'Trial' : np.repeat(np.arange(n_trials_per_dataset), len(os.listdir('../../bv')))\n",
    "        })\n",
    "    df = pd.melt(df, id_vars=['Trial'], value_vars=[c for c in df.columns if not c == 'Trial'], var_name='Method', value_name='Size of the CI')\n",
    "    \n",
    "    fig = px.box(df, x='Trial', y='Size of the CI', color='Method', title='Size of the confidence interval methods. N = {}'.format(size_sample))\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The alternative methods are more reliable than the bootstrap method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (v3.11.0:deaf509e8f, Oct 24 2022, 14:43:23) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da229f0528e1c0d1c0b9dd42b2435d18e9f382f2b763d9e722ef981a0e650149"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
